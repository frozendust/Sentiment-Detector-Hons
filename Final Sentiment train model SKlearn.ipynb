{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#imports the list of stop words.\n",
    "from nltk.corpus import stopwords\n",
    "#Used to tokenize the text into a sentace or just words.\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#imort regular exprasions\n",
    "import re\n",
    "#Import text blob for sentiment demo\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "\n",
    "stopWords = set(stopwords.words('english')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_Sentiment_Data = pd.read_csv('SentiSample_10000.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textProcess(SentiText): \n",
    "    \n",
    "    stopWords = set(stopwords.words('english')) \n",
    "    SentiText = SentiText.lower()                                     #convert to lowercase\n",
    "    \n",
    "    pattern = r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*' \n",
    "    SentiText= re.sub(pattern, \"\", SentiText)                         # remove url\n",
    "    \n",
    "    pattern = r\"[\\d]\"                           \n",
    "    SentiText= re.sub(pattern, \"\", SentiText)                         # remove numbers\n",
    "    \n",
    "    pattern = r\"[^\\w]\"\n",
    "    SentiText= re.sub(pattern, \" \", SentiText)                        # remove commas and puntication\n",
    "    \n",
    "    SentiText = SentiText.strip()                                     # remove white spaces\n",
    "    \n",
    "    SentiText = word_tokenize(SentiText)                              # tokenize sentance\n",
    "    \n",
    "    SentiText = [word for word in SentiText if not word in stopWords] # remove Stop words\n",
    "    return SentiText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 10000\n",
    "i=0\n",
    "\n",
    "count=0\n",
    "\n",
    "for count in range (length):\n",
    "    \n",
    "    sentance = textProcess(Sample_Sentiment_Data.Message[count])\n",
    "    palcer=\"\"\n",
    "    for i in (sentance):\n",
    "        palcer = palcer + \" \" + i\n",
    "    Sample_Sentiment_Data.loc[count,\"Message\"]= palcer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_Sentiment_Data = Sample_Sentiment_Data.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "Sample_Sentiment_Data_processes = pd.read_csv('SentiSample_10000_processed.csv', encoding = \"ISO-8859-1\")\n",
    "Sample_Sentiment_Data_processes = Sample_Sentiment_Data_processes.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "Sample_Sentiment_Data_processes=Sample_Sentiment_Data_processes.fillna(\"placeholder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment_data_X = Sample_Sentiment_Data_processes[['Message']]\n",
    "\n",
    "Sentiment_data_Y = Sample_Sentiment_Data_processes[['Sentiment']]\n",
    "\n",
    "#split\n",
    "from sklearn.model_selection import train_test_split\n",
    "Sentiment_data_X, X_test, Sentiment_data_Y, y_test = train_test_split(Sample_Sentiment_Data_processes[['Message']], Sample_Sentiment_Data_processes[['Sentiment']], test_size = .30, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16639)\t0.22917201315906666\n",
      "  (0, 15186)\t0.29129531619085847\n",
      "  (0, 13778)\t0.2730320398852412\n",
      "  (0, 11669)\t0.2871839077554137\n",
      "  (0, 6521)\t0.24396359128188672\n",
      "  (0, 5221)\t0.389624180226951\n",
      "  (0, 4313)\t0.263274889039997\n",
      "  (0, 3760)\t0.19238113314540017\n",
      "  (0, 3605)\t0.32499800298876563\n",
      "  (0, 3017)\t0.25414276448358253\n",
      "  (0, 2256)\t0.3362066854787182\n",
      "  (0, 1066)\t0.32358855070549614\n",
      "  (1, 15974)\t0.38731732158582943\n",
      "  (1, 10766)\t0.25059159554867605\n",
      "  (1, 8661)\t0.25700512389876645\n",
      "  (1, 5996)\t0.5073233968677964\n",
      "  (1, 5594)\t0.36477675571852836\n",
      "  (1, 2273)\t0.33364916296362734\n",
      "  (1, 1147)\t0.46837659683355676\n",
      "  (2, 9170)\t0.47165058858647535\n",
      "  (2, 6199)\t0.20920475943259828\n",
      "  (2, 5234)\t0.340866244924247\n",
      "  (2, 4716)\t0.4515568651774459\n",
      "  (2, 206)\t0.47165058858647535\n",
      "  (2, 159)\t0.43730014153344654\n",
      "  :\t:\n",
      "  (6996, 6664)\t0.33396173824454783\n",
      "  (6996, 6429)\t0.2573116577021671\n",
      "  (6996, 5670)\t0.31113901992012666\n",
      "  (6996, 4064)\t0.2887986461124082\n",
      "  (6997, 15301)\t0.3918422735216126\n",
      "  (6997, 11153)\t0.43805903167227456\n",
      "  (6997, 9820)\t0.602946537252151\n",
      "  (6997, 6199)\t0.2674422301784687\n",
      "  (6997, 4977)\t0.3425114415863718\n",
      "  (6997, 1164)\t0.3196559972329564\n",
      "  (6998, 17217)\t0.3440700431523324\n",
      "  (6998, 17188)\t0.24383516516556983\n",
      "  (6998, 14896)\t0.3440700431523324\n",
      "  (6998, 13683)\t0.23576805933768338\n",
      "  (6998, 13670)\t0.3440700431523324\n",
      "  (6998, 11104)\t0.22110964811602926\n",
      "  (6998, 9254)\t0.19854516514276516\n",
      "  (6998, 9060)\t0.3043529124339876\n",
      "  (6998, 7508)\t0.3440700431523324\n",
      "  (6998, 7507)\t0.2764004817377124\n",
      "  (6998, 6429)\t0.23957706221895217\n",
      "  (6998, 6248)\t0.31094421782775533\n",
      "  (6999, 16066)\t0.4476455546815417\n",
      "  (6999, 13492)\t0.6236210354992455\n",
      "  (6999, 11711)\t0.6408668047704636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer=TfidfVectorizer()\n",
    "vectorizer.fit(Sample_Sentiment_Data_processes.Message)\n",
    "#results=vectorizer.fit_transform([document])\n",
    "results=vectorizer.transform(Sentiment_data_X.Message)\n",
    "resultsTest=vectorizer.transform(X_test.Message)\n",
    "#results=vectorizer.fit_transform([document])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 4 0]\n"
     ]
    }
   ],
   "source": [
    "Xtrain = results\n",
    "X_test = resultsTest\n",
    "ytrain = np.array(Sentiment_data_Y)\n",
    "ytrain = np.hstack((ytrain))\n",
    "print(ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=32, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=2, gamma=0.0078125, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "grid = SVC(kernel='rbf', C=32, gamma=0.0078125, degree=2)\n",
    "grid.fit(Xtrain, ytrain)\n",
    "\n",
    "#print(\"The best parameters are %s with a score of %0.2f\"\n",
    "#      % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 954  504]\n",
      " [ 342 1200]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.65      0.69      1458\n",
      "          4       0.70      0.78      0.74      1542\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_prediction))  \n",
    "print(classification_report(y_test, y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_model_10000.sav'\n",
    "pickle.dump(grid, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.718\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "output = loaded_model.predict(X_test)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
